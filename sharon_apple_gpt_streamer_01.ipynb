{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyON83QWYRa+nhHbFNbT0cjj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/capitanalegria/capi_stream/blob/main/sharon_apple_gpt_streamer_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEe_c1Q2FEZs"
      },
      "outputs": [],
      "source": [
        "#created during stream w chat GPT3.5 API and GPT4 03/15/2023 w python 3.9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install libraries and components and run locally on an anacando or virtual env: { form-width: \"250px\", display-mode: \"form\" }\n",
        "runtime = \"local\" #@param [\"local\"]\n",
        "##@markdown Name and create project folder.\n",
        "#project_folder=\"sharon_apple\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "!pip install gdown --upgrade\n",
        "import sys\n",
        "%cd \n",
        "if runtime == 'hosted':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    work_folder='/content/drive/MyDrive/{}/'.format(project_folder)\n",
        "    !pip install --upgrade setuptools wheel\n",
        "    !sudo apt install espeak\n",
        "    !sudo apt install python3-pyaudio\n",
        "    !!pip install gtts\n",
        "    RESULTS=work_folder+'RESULTS/'\n",
        "    os.makedirs(work_folder, exist_ok=True)\n",
        "    os.makedirs(RESULTS, exist_ok=True)\n",
        "\n",
        "!pip install openai\n",
        "!pip install SpeechRecognition\n",
        "!pip install pyttsx3\n",
        "!pip install ffmpeg-python"
      ],
      "metadata": {
        "id": "1MGGcGrbFXi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run if using conda on Linux or Mac, otherwise install PyAudio from wheel matching Python version. (https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyaudio)\n",
        "!conda install -c anaconda pyaudio"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yWaj-uPD4R4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Enter your OpenAI api key\n",
        "yatu=\"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fnng2bJGRKR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Talk to your Chatgpt co-host thru mic, End convo with the words \"PLEASE STOP\"\n",
        "#@markdown Pick personality or assign one thru text, and choose to create 2 images from convo or not.  { form-width: \"100px\" }\n",
        "import webbrowser as wb\n",
        "import openai\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import wave\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "from IPython.display import Audio\n",
        "from IPython.display import display\n",
        "import urllib.request\n",
        "from tkinter import filedialog\n",
        "from tkinter import Tk\n",
        "\n",
        "make_images = True #@param {type:\"boolean\"}\n",
        "\n",
        "openai.api_key = yatu\n",
        "r = sr.Recognizer()\n",
        "prompt=\"\"\n",
        "conversation=\"\"\n",
        "response=\"\"\n",
        "image_prompt=\"\"\n",
        "reponse_summary=\"\"\n",
        "generated_image=\"\"\n",
        "\n",
        "personalidad_gpt = \"sarcastic cohost\" #@param [\"sarcastic podcast host\", \"endearing friend\", \"guiding wizard\", \"friendly chatbot\", \"dark goth boy\", \"epic gamer\", \"nerdy politician\", \"evil programmer\", \"imperial overlord\", \"egomaniacal artist\", \"rider or die partner\", \"helpful robot\"] {allow-input: true}\n",
        "\n",
        "messages=[{\"role\":\"system\", \"content\": \"you are a {}\".format(personalidad_gpt)}]\n",
        "\n",
        "# Initialize the text-to-speech engine\n",
        "engine = pyttsx3.init()\n",
        "\n",
        "# Function to recognize speech from microphone input\n",
        "def recognize_speech():\n",
        "    speech_recognition=sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        speech=''\n",
        "        print(\"Listening...\")\n",
        "        engine.say(\"Listening...\")\n",
        "        engine.runAndWait()\n",
        "        audio=speech_recognition.listen(source,phrase_time_limit=5)\n",
        "    try:\n",
        "        speech=speech_recognition.recognize_google(audio)\n",
        "        print(speech)\n",
        "        return speech\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand your speech, type request\")\n",
        "        speech=input()\n",
        "        return speech\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results; {e}\")\n",
        "        return None\n",
        "\n",
        "def interact_w_gpt():\n",
        "    global messages\n",
        "    #messages=[]\n",
        "\n",
        "    # Prompt the user to choose a directory to save the files\n",
        "    engine.say(\"Where do you want convo text files and images to be saved?...\")\n",
        "    engine.runAndWait()\n",
        "    root = Tk()\n",
        "    root.withdraw()\n",
        "    save_dir = filedialog.askdirectory(title=\"Choose a directory to save the images and text file\")\n",
        "\n",
        "    while True:\n",
        "        print(\"Speak your input:\")\n",
        "        le_prompt = recognize_speech()\n",
        "        #le_prompt=input('you:') #uncomment if you want to input prompt manually\n",
        "        if le_prompt is None:\n",
        "            continue\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": le_prompt})\n",
        "\n",
        "        response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages, temperature=0.5)\n",
        "        reply = response.choices[0].message.content\n",
        "\n",
        "        engine.say(reply)\n",
        "        print(reply)\n",
        "        engine.runAndWait()\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "        title = le_prompt[:20]\n",
        "        convo_text = os.path.join(save_dir, f'convo_{title}.txt')\n",
        "        if \"Thank you for your time\" in reply or \"please stop\" in le_prompt:\n",
        "            break\n",
        "\n",
        "        elif make_images == True:\n",
        "            image_prompt = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                temperature=0.5,\n",
        "                messages=[{\"role\": \"user\", \"content\": f'summarize {reply} to 10 words'}]\n",
        "                )\n",
        "            image_gen = image_prompt.choices[0].message.content\n",
        "            generated_image = openai.Image.create(\n",
        "                prompt=image_gen,\n",
        "                n=2, #change to however many images you want and adjust on variables below\n",
        "                size=\"1024x1024\"\n",
        "                )\n",
        "            image_urla = generated_image.data[0]['url']\n",
        "            image_urlb = generated_image.data[1]['url']\n",
        "            #image_urlc = generated_image.data[2]['url']\n",
        "            #image_urld = generated_image.data[3]['url']\n",
        "\n",
        "            urllib.request.urlretrieve(image_urla, os.path.join(save_dir, f'a{title}.jpg'))\n",
        "            urllib.request.urlretrieve(image_urlb, os.path.join(save_dir, f'b{title}.jpg'))\n",
        "            #urllib.request.urlretrieve(image_urlc, os.path.join(save_dir, f'c{title}.jpg'))\n",
        "            #urllib.request.urlretrieve(image_urld, os.path.join(save_dir, f'd{title}.jpg'))\n",
        "            with open(convo_text, \"w\") as f:\n",
        "                f.write(str(messages))\n",
        "\n",
        "        else:\n",
        "            with open(convo_text, \"w\") as f:\n",
        "                f.write(str(messages))\n",
        "\n",
        "print('Say something...')\n",
        "interact_w_gpt()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0-H_CJzW4-9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "experimental script below, not tested\n"
      ],
      "metadata": {
        "id": "Q7SqcaZWEwGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optionally use TTS AI MODEL to interpret voice input instead of google's speech recognition (not tested):\n",
        "#install TTS\n",
        "!pip install TTS\n",
        "\n",
        "#download model\n",
        "!wget https://public-asai-dl-models.s3.eu-central-1.amazonaws.com/Coqui-TTS/tts_models--en--ljspeech--tacotron2-DDC.tar.gz\n",
        "!tar -xvf tts_models--en--ljspeech--tacotron2-DDC.tar.gz"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rhGJFbea-8ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Talk to GPT w TTS and generate 2 images from convo\n",
        "\n",
        "import openai\n",
        "import urllib.request\n",
        "import os\n",
        "import speech_recognition as sr\n",
        "from tkinter import filedialog\n",
        "from tkinter import Tk\n",
        "from TTS.utils.manage import ModelManager\n",
        "from TTS.utils.synthesizer import Synthesizer\n",
        "\n",
        "# Initialize the Coqui TTS synthesizer\n",
        "model_path = \"tts_models/en/ljspeech/tacotron2-DDC\"\n",
        "config_path = os.path.join(model_path, \"config.json\")\n",
        "model_manager = ModelManager()\n",
        "tts_model, vocoder_model, tts_config, vocoder_config = model_manager.get_model_config(model_path, config_path)\n",
        "synthesizer = Synthesizer(tts_model, vocoder_model, tts_config, vocoder_config)\n",
        "\n",
        "# Function to recognize speech from microphone input\n",
        "def recognize_speech():\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Listening...\")\n",
        "        synthesizer.tts(\"Listening...\")\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    try:\n",
        "        speech = recognizer.recognize_google(audio)\n",
        "        print(f\"You said: {speech}\")\n",
        "        return speech\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand your speech\")\n",
        "        return None\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results; {e}\")\n",
        "        return None\n",
        "\n",
        "def test_audio_capture():\n",
        "    test_prompt = recognize_speech()\n",
        "    if test_prompt is not None:\n",
        "        print(\"Audio capture successful. Proceeding to run the script...\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Audio capture failed. Please check your microphone and try again.\")\n",
        "        return False\n",
        "\n",
        "personalidad_gpt = \"sarcastic cohost\" #@param [\"sarcastic podcast host\", \"endearing friend\", \"guiding wizard\", \"friendly chatbot\", \"dark goth boy\", \"epic gamer\", \"nerdy politician\", \"evil programmer\", \"imperial overlord\", \"egomaniacal artist\", \"rider or die partner\", \"helpful robot\"] {allow-input: true}\n",
        "messages=[{\"role\":\"system\", \"content\": \"you are a {}\".format(personalidad_gpt)}]\n",
        "\n",
        "def interact_w_gpt():\n",
        "    global messages\n",
        "    #message=[]\n",
        "\n",
        "    # Prompt the user to choose a directory to save the files\n",
        "    root = Tk()\n",
        "    root.withdraw()\n",
        "    save_dir = filedialog.askdirectory(title=\"Choose a directory to save the images and text file\")\n",
        "\n",
        "    while True:\n",
        "        print(\"Speak your input:\")\n",
        "        le_prompt = recognize_speech()\n",
        "        if le_prompt is None:\n",
        "            continue\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": le_prompt})\n",
        "\n",
        "        response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages, temperature=0.5)\n",
        "        reply = response.choices[0].message.content\n",
        "\n",
        "        synthesizer.tts(reply)\n",
        "        print(reply)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "        title = le_prompt[:20]\n",
        "        convo_text = os.path.join(save_dir, f'convo_{title}.txt')\n",
        "\n",
        "        image_prompt = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            temperature=0.5,\n",
        "            messages=[{\"role\": \"user\", \"content\": f'summarize {reply} to 10 words'}]\n",
        "        )\n",
        "        image_gen = image_prompt.choices[0].message.content\n",
        "\n",
        "        generated_image = openai.Image.create(\n",
        "            prompt=image_gen,\n",
        "            n=4,\n",
        "            size=\"1024x1024\"\n",
        "        )\n",
        "        image_urla = generated_image.data[0]['url']\n",
        "        image_urlb = generated_image.data[1]['url']\n",
        "\n",
        "        urllib.request.urlretrieve(image_urla, os.path.join(save_dir, f'a{title}.jpg'))\n",
        "        urllib.request.urlretrieve(image_urlb, os.path.join(save_dir, f'b{title}.jpg'))\n",
        "\n",
        "        with open(convo_text, \"w\") as f:\n",
        "            f.write(str(messages))\n",
        "\n",
        "        if \"Thank you for your time\" in reply or \"please stop\" in le_prompt:\n",
        "            break\n",
        "\n",
        "print('Say something...')\n",
        "interact_w_gpt()"
      ],
      "metadata": {
        "id": "GyzLxX0UEthx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}